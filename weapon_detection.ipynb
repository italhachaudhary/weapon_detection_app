{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ”« Weapon Detection AI**"
      ],
      "metadata": {
        "id": "pHCle4QSI3Ac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7xiTcm3ukXp",
        "outputId": "2ec46afe-8d6c-425f-ca2d-8cb2335ef385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# Install YOLOv8 (Ultralytics) and Kaggle\n",
        "!pip install ultralytics kaggle -q\n",
        "import os\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install YOLOv8 (Ultralytics)\n",
        "!pip install ultralytics -q\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# 2. Clone the repository\n",
        "!rm -rf OD-WeaponDetection  # Clean up if exists\n",
        "!git clone https://github.com/ari-dasci/OD-WeaponDetection.git\n",
        "\n",
        "print(\"Repository cloned successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RDJmr71yuSX",
        "outputId": "3bdc252b-0a5e-495b-e2a9-b5d8f735433f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Cloning into 'OD-WeaponDetection'...\n",
            "remote: Enumerating objects: 50755, done.\u001b[K\n",
            "remote: Total 50755 (delta 0), reused 0 (delta 0), pack-reused 50755 (from 1)\u001b[K\n",
            "Receiving objects: 100% (50755/50755), 2.26 GiB | 40.19 MiB/s, done.\n",
            "Resolving deltas: 100% (10983/10983), done.\n",
            "Updating files: 100% (63128/63128), done.\n",
            "Repository cloned successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XdvfzpCrzAzA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9df32a49"
      },
      "source": [
        "# Task\n",
        "Prepare the dataset for YOLOv8 by converting existing annotations into YOLO format, normalizing bounding box coordinates, and splitting the data into training, validation, and test sets, then organizing them within the `/content/yolo_dataset` directory from the `OD-WeaponDetection` repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc85821e"
      },
      "source": [
        "## Prepare Dataset for YOLOv8\n",
        "\n",
        "### Subtask:\n",
        "Process raw dataset by converting existing XML annotations to YOLO format with normalized bounding box coordinates, and then organizing images and labels into training, validation, and test sets within the '/content/yolo_dataset' directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c508bd09"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to define the `class_map` to assign integer IDs to the object classes, and then set up the necessary directory structure for the YOLO dataset, including train, validation, and test subdirectories for both images and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca75a5e",
        "outputId": "05ea0782-da68-4796-dc32-20abce10927d"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Define class_map\n",
        "class_map = {'pistol': 0, 'knife': 1}\n",
        "print(f\"Class map defined: {class_map}\")\n",
        "\n",
        "# 2. Define output directory and create subdirectories\n",
        "output_dir = '/content/yolo_dataset'\n",
        "\n",
        "# Clean up previous runs if any\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "os.makedirs(os.path.join(output_dir, 'images', 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'images', 'val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'images', 'test'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'labels', 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'labels', 'val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'labels', 'test'), exist_ok=True)\n",
        "print(f\"Created YOLO dataset directory structure at: {output_dir}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class map defined: {'pistol': 0, 'knife': 1}\n",
            "Created YOLO dataset directory structure at: /content/yolo_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b1062e5"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `convert_to_yolo` function as described in the instructions. This function will parse XML annotation files, convert bounding box coordinates to the normalized YOLO format, and use the `class_map` to assign integer IDs to classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8cd38ed",
        "outputId": "8ef49222-de8c-4947-83a4-4b6e841ff001"
      },
      "source": [
        "def convert_to_yolo(xml_file_path, img_width, img_height, class_map):\n",
        "    yolo_annotations = []\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        if class_name not in class_map:\n",
        "            print(f\"Warning: Class '{class_name}' not in class_map. Skipping object.\")\n",
        "            continue\n",
        "        class_id = class_map[class_name]\n",
        "\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = int(bndbox.find('xmin').text)\n",
        "        ymin = int(bndbox.find('ymin').text)\n",
        "        xmax = int(bndbox.find('xmax').text)\n",
        "        ymax = int(bndbox.find('ymax').text)\n",
        "\n",
        "        # Convert to YOLO format (x_center, y_center, width, height) normalized\n",
        "        x_center = (xmin + xmax) / 2 / img_width\n",
        "        y_center = (ymin + ymax) / 2 / img_height\n",
        "        width = (xmax - xmin) / img_width\n",
        "        height = (ymax - ymin) / img_height\n",
        "\n",
        "        yolo_annotations.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
        "\n",
        "    return yolo_annotations\n",
        "\n",
        "print(\"Defined 'convert_to_yolo' function.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 'convert_to_yolo' function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80769b18"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to initialize an empty list `all_image_paths` and then iterate through the `source_folders`. For each folder, the code will find all image and XML annotation file pairs, extract image dimensions, convert XML annotations to YOLO format using the `convert_to_yolo` function, save the YOLO annotations to `.txt` files, and finally populate the `all_image_paths` list with tuples of image and label file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f00552d9",
        "outputId": "4c5158e7-e25f-40e5-f174-a5377c15a5fe"
      },
      "source": [
        "all_image_paths = []\n",
        "\n",
        "# Source folders containing images and XML annotations\n",
        "source_folders = [\n",
        "    '/content/OD-WeaponDetection/Pistol detection',\n",
        "    '/content/OD-WeaponDetection/Knife_detection'\n",
        "]\n",
        "\n",
        "# Create a temporary directory for YOLO labels before splitting\n",
        "temp_label_dir = os.path.join(output_dir, 'temp_yolo_labels')\n",
        "os.makedirs(temp_label_dir, exist_ok=True)\n",
        "\n",
        "for folder in source_folders:\n",
        "    print(f\"Processing folder: {folder}\")\n",
        "    image_files = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "        image_files.extend(glob.glob(os.path.join(folder, ext)))\n",
        "\n",
        "    for img_path in image_files:\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        xml_path = os.path.join(folder, base_name + '.xml')\n",
        "\n",
        "        if os.path.exists(xml_path):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    img_width, img_height = img.size\n",
        "\n",
        "                yolo_annotations = convert_to_yolo(xml_path, img_width, img_height, class_map)\n",
        "\n",
        "                # Save YOLO annotations to a temporary .txt file\n",
        "                yolo_label_path = os.path.join(temp_label_dir, base_name + '.txt')\n",
        "                with open(yolo_label_path, 'w') as f:\n",
        "                    for line in yolo_annotations:\n",
        "                        f.write(line + '\\n')\n",
        "\n",
        "                all_image_paths.append((img_path, yolo_label_path))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path} or {xml_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: XML file not found for {img_path}. Skipping.\")\n",
        "\n",
        "print(f\"Total image-annotation pairs processed: {len(all_image_paths)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing folder: /content/OD-WeaponDetection/Pistol detection\n",
            "Processing folder: /content/OD-WeaponDetection/Knife_detection\n",
            "Total image-annotation pairs processed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b756ab3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution processed 0 image-annotation pairs, indicating an issue with file path detection. The problem likely stems from incorrect paths for images and XML annotations. I need to re-examine the directory structure and correct the `glob.glob` pattern to accurately locate the image and XML files, which are typically within an 'images' subdirectory inside the `source_folders`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a0b6b5a",
        "outputId": "7e8a46fb-d855-418f-eb45-bc740c24c570"
      },
      "source": [
        "all_image_paths = []\n",
        "\n",
        "# Source folders containing images and XML annotations\n",
        "source_folders = [\n",
        "    '/content/OD-WeaponDetection/Pistol detection',\n",
        "    '/content/OD-WeaponDetection/Knife_detection'\n",
        "]\n",
        "\n",
        "# Create a temporary directory for YOLO labels before splitting\n",
        "temp_label_dir = os.path.join(output_dir, 'temp_yolo_labels')\n",
        "os.makedirs(temp_label_dir, exist_ok=True)\n",
        "\n",
        "for folder in source_folders:\n",
        "    print(f\"Processing folder: {folder}\")\n",
        "\n",
        "    # Images and XMLs are likely in a subdirectory named 'images' within the main folder\n",
        "    image_subfolder = os.path.join(folder, 'images')\n",
        "\n",
        "    if not os.path.exists(image_subfolder):\n",
        "        print(f\"Warning: Image subfolder not found at {image_subfolder}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    image_files = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "        image_files.extend(glob.glob(os.path.join(image_subfolder, ext)))\n",
        "\n",
        "    for img_path in image_files:\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "        # Assuming XMLs are in the same 'images' subfolder as the images\n",
        "        xml_path = os.path.join(image_subfolder, base_name + '.xml')\n",
        "\n",
        "        if os.path.exists(xml_path):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    img_width, img_height = img.size\n",
        "\n",
        "                yolo_annotations = convert_to_yolo(xml_path, img_width, img_height, class_map)\n",
        "\n",
        "                # Save YOLO annotations to a temporary .txt file\n",
        "                yolo_label_path = os.path.join(temp_label_dir, base_name + '.txt')\n",
        "                with open(yolo_label_path, 'w') as f:\n",
        "                    for line in yolo_annotations:\n",
        "                        f.write(line + '\\n')\n",
        "\n",
        "                all_image_paths.append((img_path, yolo_label_path))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path} or {xml_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: XML file not found for {img_path}. Skipping.\")\n",
        "\n",
        "print(f\"Total image-annotation pairs processed: {len(all_image_paths)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing folder: /content/OD-WeaponDetection/Pistol detection\n",
            "Warning: Image subfolder not found at /content/OD-WeaponDetection/Pistol detection/images. Skipping.\n",
            "Processing folder: /content/OD-WeaponDetection/Knife_detection\n",
            "Warning: Image subfolder not found at /content/OD-WeaponDetection/Knife_detection/images. Skipping.\n",
            "Total image-annotation pairs processed: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13848a38"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to locate image and XML files within `source_folders` have failed, indicating my assumptions about the directory structure are incorrect. I need to inspect the contents of the `OD-WeaponDetection` repository to identify the correct paths to the image and annotation files. I'll use a shell command to list the files and subdirectories within the `source_folders` to understand the actual layout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ebb065",
        "outputId": "92427bbe-bef5-45fc-dbb3-c97439e22ca5"
      },
      "source": [
        "import os\n",
        "\n",
        "# Inspect the contents of the source folders\n",
        "for folder in source_folders:\n",
        "    print(f\"Listing contents of {folder}:\")\n",
        "    !ls -F \"{folder}\"\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Also inspect a bit deeper into one of the likely candidates if they exist, e.g., 'images' or 'annotations' within the source folder\n",
        "# Let's try inspecting '/content/OD-WeaponDetection/Pistol detection/Images' and '/content/OD-WeaponDetection/Pistol detection/Annotations'\n",
        "\n",
        "pistol_images_path = os.path.join(source_folders[0], 'Images')\n",
        "pistol_annotations_path = os.path.join(source_folders[0], 'Annotations')\n",
        "knife_images_path = os.path.join(source_folders[1], 'Images')\n",
        "knife_annotations_path = os.path.join(source_folders[1], 'Annotations')\n",
        "\n",
        "if os.path.exists(pistol_images_path):\n",
        "    print(f\"Listing contents of {pistol_images_path}:\")\n",
        "    !ls -F \"{pistol_images_path}\" | head -n 5\n",
        "    print(\"(Showing first 5 items)\\n\")\n",
        "\n",
        "if os.path.exists(pistol_annotations_path):\n",
        "    print(f\"Listing contents of {pistol_annotations_path}:\")\n",
        "    !ls -F \"{pistol_annotations_path}\" | head -n 5\n",
        "    print(\"(Showing first 5 items)\\n\")\n",
        "\n",
        "if os.path.exists(knife_images_path):\n",
        "    print(f\"Listing contents of {knife_images_path}:\")\n",
        "    !ls -F \"{knife_images_path}\" | head -n 5\n",
        "    print(\"(Showing first 5 items)\\n\")\n",
        "\n",
        "if os.path.exists(knife_annotations_path):\n",
        "    print(f\"Listing contents of {knife_annotations_path}:\")\n",
        "    !ls -F \"{knife_annotations_path}\" | head -n 5\n",
        "    print(\"(Showing first 5 items)\\n\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of /content/OD-WeaponDetection/Pistol detection:\n",
            "Weapons/  xmls/\n",
            "\n",
            "\n",
            "Listing contents of /content/OD-WeaponDetection/Knife_detection:\n",
            "annotations/  Images/\n",
            "\n",
            "\n",
            "Listing contents of /content/OD-WeaponDetection/Knife_detection/Images:\n",
            "DefenseAndSurvive02.jpg*\n",
            "DefenseAndSurvive04.jpg*\n",
            "DefenseAndSurvive05.jpg*\n",
            "DefenseAndSurvive07.jpg*\n",
            "DefenseAndSurvive09.jpg*\n",
            "(Showing first 5 items)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106b0b90"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the inspection of the directory structure, the image and XML annotation files are located in different subdirectories for 'Pistol detection' and 'Knife_detection'. I need to adjust the code to correctly identify these paths and process the image-annotation pairs accordingly to populate `all_image_paths`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b71eff0c",
        "outputId": "0787822a-d96f-4c64-9eaa-26ad2943834e"
      },
      "source": [
        "all_image_paths = []\n",
        "\n",
        "# Source folders containing images and XML annotations\n",
        "source_folders = {\n",
        "    'pistol': {'base': '/content/OD-WeaponDetection/Pistol detection', 'images': 'Weapons', 'annotations': 'xmls'},\n",
        "    'knife': {'base': '/content/OD-WeaponDetection/Knife_detection', 'images': 'Images', 'annotations': 'annotations'}\n",
        "}\n",
        "\n",
        "# Create a temporary directory for YOLO labels before splitting\n",
        "temp_label_dir = os.path.join(output_dir, 'temp_yolo_labels')\n",
        "os.makedirs(temp_label_dir, exist_ok=True)\n",
        "\n",
        "for weapon_type, paths in source_folders.items():\n",
        "    print(f\"Processing {weapon_type} detection folder: {paths['base']}\")\n",
        "\n",
        "    image_source_path = os.path.join(paths['base'], paths['images'])\n",
        "    annotation_source_path = os.path.join(paths['base'], paths['annotations'])\n",
        "\n",
        "    if not os.path.exists(image_source_path):\n",
        "        print(f\"Warning: Image folder not found at {image_source_path}. Skipping.\")\n",
        "        continue\n",
        "    if not os.path.exists(annotation_source_path):\n",
        "        print(f\"Warning: Annotation folder not found at {annotation_source_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    image_files = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "        image_files.extend(glob.glob(os.path.join(image_source_path, ext)))\n",
        "\n",
        "    for img_path in image_files:\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "        xml_path = os.path.join(annotation_source_path, base_name + '.xml')\n",
        "\n",
        "        if os.path.exists(xml_path):\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    img_width, img_height = img.size\n",
        "\n",
        "                yolo_annotations = convert_to_yolo(xml_path, img_width, img_height, class_map)\n",
        "\n",
        "                # Save YOLO annotations to a temporary .txt file\n",
        "                yolo_label_path = os.path.join(temp_label_dir, base_name + '.txt')\n",
        "                with open(yolo_label_path, 'w') as f:\n",
        "                    for line in yolo_annotations:\n",
        "                        f.write(line + '\\n')\n",
        "\n",
        "                all_image_paths.append((img_path, yolo_label_path))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path} or {xml_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: XML file not found for {img_path}. Skipping.\")\n",
        "\n",
        "print(f\"Total image-annotation pairs processed: {len(all_image_paths)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing pistol detection folder: /content/OD-WeaponDetection/Pistol detection\n",
            "Processing knife detection folder: /content/OD-WeaponDetection/Knife_detection\n",
            "Total image-annotation pairs processed: 4948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12c7c342"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to split the `all_image_paths` into training, validation, and test sets using `train_test_split` from `sklearn.model_selection`. This involves two splits to achieve the desired 70/20/10 ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "611fe07f",
        "outputId": "66f1c2f9-6df2-4daf-ff8c-22ef11dc8397"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training (70%) and temp (30% for val+test)\n",
        "train_paths, temp_paths = train_test_split(all_image_paths, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split temp into validation (20%) and test (10%)\n",
        "# 0.6666... because 20% of 100% is 2/3 of 30%\n",
        "val_paths, test_paths = train_test_split(temp_paths, test_size=0.3333333, random_state=42)\n",
        "\n",
        "print(f\"Total images: {len(all_image_paths)}\")\n",
        "print(f\"Training images: {len(train_paths)}\")\n",
        "print(f\"Validation images: {len(val_paths)}\")\n",
        "print(f\"Test images: {len(test_paths)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 4948\n",
            "Training images: 3463\n",
            "Validation images: 990\n",
            "Test images: 495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0d3b07"
      },
      "source": [
        "**Reasoning**:\n",
        "The final step is to copy the images and their corresponding YOLO label files from the temporary locations to their designated train, validation, and test directories within the `output_dir`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8830aa1",
        "outputId": "95496e3b-07b4-48f9-9010-5082e7323aa5"
      },
      "source": [
        "dataset_splits = {\n",
        "    'train': train_paths,\n",
        "    'val': val_paths,\n",
        "    'test': test_paths\n",
        "}\n",
        "\n",
        "for split, paths_list in dataset_splits.items():\n",
        "    print(f\"Copying files for {split} split...\")\n",
        "    for img_src, label_src in paths_list:\n",
        "        # Determine destination paths\n",
        "        img_dst_dir = os.path.join(output_dir, 'images', split)\n",
        "        label_dst_dir = os.path.join(output_dir, 'labels', split)\n",
        "\n",
        "        # Copy image file\n",
        "        shutil.copy(img_src, os.path.join(img_dst_dir, os.path.basename(img_src)))\n",
        "\n",
        "        # Copy label file\n",
        "        shutil.copy(label_src, os.path.join(label_dst_dir, os.path.basename(label_src)))\n",
        "\n",
        "print(\"Dataset preparation complete. Files are organized in '/content/yolo_dataset'.\")\n",
        "\n",
        "# Clean up the temporary label directory\n",
        "shutil.rmtree(temp_label_dir)\n",
        "print(f\"Cleaned up temporary label directory: {temp_label_dir}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files for train split...\n",
            "Copying files for val split...\n",
            "Copying files for test split...\n",
            "Dataset preparation complete. Files are organized in '/content/yolo_dataset'.\n",
            "Cleaned up temporary label directory: /content/yolo_dataset/temp_yolo_labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f30bf52"
      },
      "source": [
        "## Create YOLOv8 Configuration File\n",
        "\n",
        "### Subtask:\n",
        "Generate the 'data.yaml' file, which is crucial for YOLOv8 training. This file will define the paths to your training, validation, and test image directories and list the names of the object classes (e.g., 'pistol', 'knife') that your model will learn to detect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f15ac16"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask, I need to generate the 'data.yaml' file as specified in the instructions. This involves defining the paths for training, validation, and testing images, determining the number of classes, listing class names from the `class_map`, and then writing this information in YAML format to the `data.yaml` file within the `output_dir`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "162039a5",
        "outputId": "e42054d2-c0dc-46f7-9e7b-b71c6665e646"
      },
      "source": [
        "import os\n",
        "\n",
        "# 1. Define paths for training, validation, and test image directories\n",
        "train_path = os.path.join('images', 'train')\n",
        "val_path = os.path.join('images', 'val')\n",
        "test_path = os.path.join('images', 'test')\n",
        "\n",
        "# 2. Define the number of classes (nc)\n",
        "nc = len(class_map)\n",
        "\n",
        "# 3. Define the class names (names) in order\n",
        "# Sort class_map by values (IDs) to ensure correct order\n",
        "names = [k for k, v in sorted(class_map.items(), key=lambda item: item[1])]\n",
        "\n",
        "# 4. Create the data.yaml file path\n",
        "data_yaml_path = os.path.join(output_dir, 'data.yaml')\n",
        "\n",
        "# 5. Write the defined paths, number of classes, and class names into the data.yaml file\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    f.write(f\"path: {output_dir}\\n\") # Base path for dataset\n",
        "    f.write(f\"train: {train_path}\\n\")\n",
        "    f.write(f\"val: {val_path}\\n\")\n",
        "    f.write(f\"test: {test_path}\\n\")\n",
        "    f.write(f\"\\nnc: {nc}\\n\")\n",
        "    f.write(f\"names: {names}\\n\")\n",
        "\n",
        "print(f\"'data.yaml' created successfully at {data_yaml_path}:\")\n",
        "with open(data_yaml_path, 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.yaml' created successfully at /content/yolo_dataset/data.yaml:\n",
            "path: /content/yolo_dataset\n",
            "train: images/train\n",
            "val: images/val\n",
            "test: images/test\n",
            "\n",
            "nc: 2\n",
            "names: ['pistol', 'knife']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1bc6c35"
      },
      "source": [
        "## Train YOLOv8 Model\n",
        "\n",
        "### Subtask:\n",
        "Initialize a YOLOv8 object detection model and begin the training process. The model will be trained using the prepared dataset and the configurations specified in the 'data.yaml' file. This step will also include setting training parameters such as the number of epochs and image size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c6b7fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires initializing a YOLOv8 model and starting the training process with specified parameters, using the previously created 'data.yaml' file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d18222da",
        "outputId": "50219363-5cc5-4b9f-99f8-97ea0b056a72"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# 1. Instantiate a YOLOv8 model (e.g., yolov8n.pt)\n",
        "model = YOLO('yolov8n.pt')\n",
        "print(\"YOLOv8n model loaded.\")\n",
        "\n",
        "# 2. Start the training process\n",
        "# 3. Pass the path to the data.yaml file\n",
        "# 4. Set the number of training epochs to 50\n",
        "# 5. Set the image size for training to 640\n",
        "results = model.train(data=data_yaml_path, epochs=50, imgsz=640)\n",
        "\n",
        "print(\"Model training initiated.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 99.2MB/s 0.1s\n",
            "YOLOv8n model loaded.\n",
            "Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/yolo_dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 25.3MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 105.3MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1546.5Â±1739.3 MB/s, size: 84.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/labels/train... 3463 images, 0 backgrounds, 1 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3463/3463 2.1Kit/s 1.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (1125).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (1217).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (1717).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2815).jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.8726      1.6186]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2843).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2902).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2907).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2910).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2911).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (2920).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (583).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/armas (612).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/images/train/knife_71.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 500.0Â±336.8 MB/s, size: 138.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/labels/val... 990 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 990/990 967.2it/s 1.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/yolo_dataset/images/val/armas (1221).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/yolo_dataset/images/val/armas (1759).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/yolo_dataset/images/val/armas (2140).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/yolo_dataset/images/val/armas (55).jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/labels/val.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.09G      1.224      2.225       1.45         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.1it/s 1:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 2.8it/s 11.3s\n",
            "                   all        990       1136      0.609      0.505      0.507      0.277\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50      2.95G      1.312       1.84      1.525         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.5s\n",
            "                   all        990       1136      0.382        0.4      0.297      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      2.97G      1.351      1.716      1.563         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.3s\n",
            "                   all        990       1136      0.639      0.459      0.533      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      2.98G      1.342      1.626      1.568         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.9it/s 7.9s\n",
            "                   all        990       1136      0.659      0.468      0.521      0.309\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50         3G      1.291      1.512      1.498         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.4s\n",
            "                   all        990       1136       0.71       0.55      0.652      0.393\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      3.02G      1.217      1.384      1.452         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.1it/s 10.1s\n",
            "                   all        990       1136      0.708       0.63      0.692      0.417\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      3.04G      1.209       1.34      1.436         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.2s\n",
            "                   all        990       1136      0.721      0.643      0.722      0.434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      3.05G      1.146      1.264      1.388         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.9it/s 8.0s\n",
            "                   all        990       1136      0.779      0.612      0.717      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      3.07G      1.143      1.234      1.398         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.1it/s 10.1s\n",
            "                   all        990       1136      0.802      0.663      0.774      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      3.09G      1.122      1.184      1.377         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.6s\n",
            "                   all        990       1136      0.805      0.697      0.798      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      3.11G       1.12      1.172      1.361         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.6it/s 8.6s\n",
            "                   all        990       1136      0.804       0.69      0.785      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      3.12G      1.091      1.115      1.345         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.2it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 4.0it/s 7.8s\n",
            "                   all        990       1136      0.816      0.754      0.836      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      3.14G      1.073      1.077      1.335         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.6it/s 8.7s\n",
            "                   all        990       1136      0.852      0.751       0.85      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      3.15G      1.075      1.065      1.333         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.7s\n",
            "                   all        990       1136      0.815      0.736      0.844      0.555\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      3.18G      1.051      1.031      1.317         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.3s\n",
            "                   all        990       1136      0.844      0.764      0.856      0.564\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      3.19G      1.038      1.009      1.298         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.5s\n",
            "                   all        990       1136      0.891      0.752      0.857      0.581\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      3.21G      1.002     0.9693      1.278         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.9it/s 8.0s\n",
            "                   all        990       1136      0.882       0.77      0.868      0.582\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      3.22G      1.008     0.9642       1.28         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.6it/s 8.7s\n",
            "                   all        990       1136      0.844      0.793      0.873      0.581\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      3.24G          1      0.939      1.275         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.7s\n",
            "                   all        990       1136      0.889      0.778      0.882      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      3.26G     0.9802     0.9148      1.263         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.7s\n",
            "                   all        990       1136      0.899      0.769      0.886      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      3.28G     0.9935     0.9199      1.266         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.1s\n",
            "                   all        990       1136      0.887      0.786      0.884      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      3.29G     0.9653     0.8888      1.256         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.2it/s 1:08\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 4.0it/s 7.8s\n",
            "                   all        990       1136      0.873      0.812      0.896      0.624\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      3.31G     0.9686     0.8765      1.255         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.5it/s 8.9s\n",
            "                   all        990       1136      0.915      0.791       0.89      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      3.33G     0.9389     0.8542      1.235         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.6s\n",
            "                   all        990       1136      0.874      0.818      0.897      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      3.34G     0.9462     0.8466      1.232         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.1it/s 9.9s\n",
            "                   all        990       1136      0.887      0.818      0.899      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      3.36G     0.9362      0.828      1.227         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 4.0it/s 7.8s\n",
            "                   all        990       1136      0.925      0.813      0.911       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      3.38G      0.923     0.8029      1.217         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.2it/s 1:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.5s\n",
            "                   all        990       1136      0.907       0.82      0.914      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      3.39G      0.914      0.784      1.215          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.5s\n",
            "                   all        990       1136      0.891      0.769      0.879      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      3.41G     0.9041     0.7824       1.21         13        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.5it/s 8.8s\n",
            "                   all        990       1136      0.888       0.81      0.901      0.643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      3.42G      0.898     0.7663      1.198         21        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.9it/s 7.9s\n",
            "                   all        990       1136      0.876       0.85      0.912      0.651\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      3.45G     0.8786     0.7503      1.192         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.1s\n",
            "                   all        990       1136      0.907      0.822      0.913      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      3.46G     0.8807     0.7353      1.184         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.1s\n",
            "                   all        990       1136      0.901      0.834      0.912       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      3.48G     0.8731     0.7236      1.179         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.2s\n",
            "                   all        990       1136      0.877      0.843      0.907      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      3.49G     0.8681     0.7297      1.182         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 4.0it/s 7.8s\n",
            "                   all        990       1136      0.919      0.821      0.921      0.657\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      3.52G     0.8541     0.7024      1.167         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.9it/s 7.9s\n",
            "                   all        990       1136      0.919      0.828      0.923      0.667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      3.53G     0.8494     0.6958      1.162         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.7s\n",
            "                   all        990       1136      0.887      0.868      0.924      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      3.55G     0.8445     0.6886      1.159         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.2s\n",
            "                   all        990       1136      0.919      0.847      0.919      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      3.56G     0.8393     0.6627      1.152         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.5it/s 8.7s\n",
            "                   all        990       1136       0.93      0.829       0.92      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      3.58G     0.8195     0.6591      1.146         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.3it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 4.0it/s 7.8s\n",
            "                   all        990       1136      0.905      0.857      0.918       0.67\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50       3.6G     0.8138     0.6446       1.14         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.5it/s 8.8s\n",
            "                   all        990       1136       0.93      0.853      0.926      0.668\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      3.62G      0.789     0.5441      1.127          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.6s\n",
            "                   all        990       1136        0.9      0.846      0.917      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      3.63G     0.7655     0.5117      1.104          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:02\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 2.9it/s 10.6s\n",
            "                   all        990       1136      0.904      0.855      0.918       0.67\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      3.65G     0.7531     0.4897      1.101          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.6it/s 1:01\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.6s\n",
            "                   all        990       1136      0.898      0.859      0.916      0.668\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      3.67G     0.7268      0.475      1.079          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:01\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.4s\n",
            "                   all        990       1136      0.901      0.857      0.921      0.676\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      3.68G     0.7249     0.4615      1.076          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:02\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.1s\n",
            "                   all        990       1136      0.934      0.853      0.926      0.679\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50       3.7G     0.7236      0.455      1.071          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:01\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.1s\n",
            "                   all        990       1136      0.929      0.864      0.929      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      3.72G     0.7096     0.4431      1.059          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:02\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.3it/s 9.4s\n",
            "                   all        990       1136      0.932      0.862      0.926       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      3.73G     0.6984     0.4279      1.061          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.4it/s 1:03\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.2s\n",
            "                   all        990       1136      0.914      0.859      0.929      0.683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      3.75G     0.6866     0.4199      1.047          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.5it/s 1:02\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.4it/s 9.3s\n",
            "                   all        990       1136      0.929      0.864      0.934       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      3.77G     0.6759     0.4088      1.047          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 217/217 3.6it/s 1:01\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.2it/s 9.6s\n",
            "                   all        990       1136      0.927       0.86      0.931      0.689\n",
            "\n",
            "50 epochs completed in 1.033 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.232 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 3.5it/s 8.8s\n",
            "                   all        990       1136      0.928      0.865      0.933       0.69\n",
            "                pistol        609        743      0.904      0.808      0.896      0.691\n",
            "                 knife        381        393      0.953      0.921       0.97      0.689\n",
            "Speed: 0.2ms preprocess, 1.8ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Model training initiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(f'/content/runs/detect/train/weights/best.pt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2eCoB2DXD8cI",
        "outputId": "15cea5d0-6be7-48df-97bb-84e69e585941"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea8c042f-1e61-403e-8689-6418e87ec3e3\", \"best.pt\", 6230698)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}